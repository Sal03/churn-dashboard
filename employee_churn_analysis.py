# -*- coding: utf-8 -*-
"""Employee_Churn_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aP-jo0Z9fGZ4RmPLhcPHXAZpggIX6h2O
"""

from google.colab import auth
auth.authenticate_user()

from google.cloud import bigquery
import pandas as pd

project_id = "employee-churn-59343"
client = bigquery.Client(project=project_id)

query = """
SELECT
  employee_id, satisfaction_level, last_evaluation, number_project,
  average_montly_hours AS average_monthly_hours,
  time_spend_company, Work_accident, promotion_last_5years,
  Departments, salary, Quit_the_Company
FROM
  `employee-churn-59343.employeedata.tbl_full_data`
WHERE
  employee_id IS NOT NULL
"""

df = client.query(query).to_dataframe()
print("‚úÖ Loaded from BigQuery:", df.shape)
df.head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import joblib

# Step 1: Rename column if needed (typo fix)
if "average_montly_hours" in df.columns:
    df.rename(columns={"average_montly_hours": "average_monthly_hours"}, inplace=True)

# Step 2: Label encode categorical features
dept_encoder = LabelEncoder()
df["Departments"] = dept_encoder.fit_transform(df["Departments"])

sal_encoder = LabelEncoder()
df["salary"] = sal_encoder.fit_transform(df["salary"])

# Step 3: Define feature order (this must be preserved for streaming)
feature_order = [
    "satisfaction_level", "last_evaluation", "number_project",
    "average_monthly_hours", "time_spend_company", "Work_accident",
    "promotion_last_5years", "Departments", "salary"
]

# Step 4: Define features (X) and target (y)
X = df[feature_order]
y = df["Quit_the_Company"]

# Step 5: Train/test split and train model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 6: Save everything needed for streaming
joblib.dump(model, "churn_model.pkl")
joblib.dump(dept_encoder, "dept_encoder.pkl")
joblib.dump(sal_encoder, "sal_encoder.pkl")
joblib.dump(feature_order, "feature_order.pkl")

print("‚úÖ Model and all files saved successfully!")

from google.cloud import pubsub_v1, bigquery
import joblib, json, pandas as pd
from datetime import datetime
import logging

# üìå Set project and BigQuery table
PROJECT_ID = "employee-churn-59343"
BQ_TABLE = "employee-churn-59343.employeedata.prediction_results"

# ‚úÖ Load trained model and saved encoders
model = joblib.load("churn_model.pkl")
dept_encoder = joblib.load("dept_encoder.pkl")
sal_encoder = joblib.load("sal_encoder.pkl")
feature_order = joblib.load("feature_order.pkl")

# ‚úÖ Initialize BigQuery client
bq_client = bigquery.Client(project=PROJECT_ID)

# üß† Define the callback for each incoming message
def callback(message):
    try:
        data = json.loads(message.data.decode("utf-8"))
        employee_id = data.get("employee_id", "unknown")

        # üîÑ Prepare DataFrame for prediction
        df = pd.DataFrame([{
            "satisfaction_level": data.get("satisfaction_level", 0),
            "last_evaluation": data.get("last_evaluation", 0),
            "number_project": data.get("number_project", 0),
            "average_monthly_hours": data.get("average_monthly_hours", 0),
            "time_spend_company": data.get("time_spend_company", 0),
            "Work_accident": data.get("Work_accident", 0),
            "promotion_last_5years": data.get("promotion_last_5years", 0),
            "Departments": data.get("Departments", "Unknown"),
            "salary": data.get("salary", "low")
        }])

        # üéØ Encode categorical values
        df["Departments"] = dept_encoder.transform([df.at[0, "Departments"]])
        df["salary"] = sal_encoder.transform([df.at[0, "salary"]])

        # üìå Ensure correct column order
        df = df[feature_order]

        # üß† Make prediction
        prob = model.predict_proba(df)[:, 1][0]
        label = 1 if prob >= 0.5 else 0

        # ‚¨ÜÔ∏è Insert result into BigQuery
        row = {
            "employee_id": employee_id,
            "predicted_churn_label": label,
            "churn_probability": round(prob, 4),
            "prediction_time": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        }

        errors = bq_client.insert_rows_json(BQ_TABLE, [row])
        if errors:
            logging.error(f"‚ùå BigQuery error: {errors}")
        else:
            print(f"‚úÖ Prediction saved for {employee_id}")

        message.ack()

    except Exception as e:
        logging.error(f"‚ùå Error processing message: {e}")
        message.nack()

# üöÄ Start subscriber
subscriber = pubsub_v1.SubscriberClient()
subscription_path = subscriber.subscription_path(PROJECT_ID, "employee-events-sub")
subscriber.subscribe(subscription_path, callback=callback)

print("üì° Listening for live employee events... (Press STOP to exit)")
import time
while True:
    time.sleep(10)

from google.colab import auth
auth.authenticate_user()

from google.cloud import pubsub_v1, bigquery
import pandas as pd
import json
import time

# üìå Set your project and topic
project_id = "employee-churn-59343"
topic_id = "employee-events"
publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path(project_id, topic_id)

# ‚úÖ Connect to BigQuery and load records
bq_client = bigquery.Client(project=project_id)

query = """
SELECT
  employee_id, satisfaction_level, last_evaluation, number_project,
  average_montly_hours AS average_monthly_hours,
  time_spend_company, Work_accident, promotion_last_5years,
  Departments, salary, Quit_the_Company
FROM
  `employee-churn-59343.employeedata.tbl_full_data`
WHERE
  employee_id IS NOT NULL
LIMIT 50
"""

df = bq_client.query(query).to_dataframe()

# üì° Stream rows as messages to Pub/Sub
for _, row in df.iterrows():
    message_json = json.dumps(row.to_dict()).encode("utf-8")
    future = publisher.publish(topic_path, data=message_json)
    print("üì§ Sent:", row.get("employee_id", "N/A"))
    time.sleep(1)  # Simulate real-time events

"""[link text](https://)#Connect to Big Query"""

#libraries that we need
from google.cloud import bigquery
from google.colab import auth

#authenticate
auth.authenticate_user()

#initialize the client for BigQuery
project_id = 'employee-churn-59343'
client = bigquery.Client(project=project_id, location='US')

#get the dataset and table
dataset_ref = client.dataset('employeedata', project=project_id)
dataset = client.get_dataset(dataset_ref)
table_ref = dataset.table('tbl_hr_data')
table = client.get_table(table_ref)
table.schema

new_table_ref = dataset.table('tbl_new_employees')
new_table = client.get_table(new_table_ref)
new_table.schema

# convert to dataframe

df =client.list_rows(table=table).to_dataframe()
df.head()

# convert to dataframe

df2 =client.list_rows(table=new_table).to_dataframe()
df2.head()

"""# Build Model

## Install Pycaret
"""

!pip install pycaret

"""# Code and Train Model"""

# get our model
from pycaret.classification import *

df.info()

df.columns

# setup or model
setup(df, target='Quit_the_Company',
      session_id=123, ignore_features=['employee_id'], categorical_features=['salary','Departments'])

compare_models()

rf_model = create_model('rf')

final_df = predict_model(rf_model)

final_df.head()

new_predictions = predict_model(rf_model, data = df2)

new_predictions.head()

# write back to bigquery
new_predictions.to_gbq('employeedata.pilot_predictions',
                       project_id,
                       chunksize = None, if_exists='replace')

plot_model(rf_model, plot='feature')

# create a feature table

rf_model.feature_names_in_

rf_model.feature_importances_

import pandas as pd
feature_table = pd.DataFrame(zip(rf_model.feature_names_in_, rf_model.feature_importances_),
                             columns = ['feature', 'importance'])
feature_table

feature_table.to_gbq('employeedata.feature_table',
                     project_id,
                     chunksize = None,
                     if_exists = 'replace')